# GenAI Risk Assessment Tool (v0.1)

A Python-based learning project exploring Generative AI security risks using concepts from the OWASP GenAI Security Project.

This repository documents my hands-on journey combining Python fundamentals, risk thinking, and GenAI security governance.

## Purpose
- Practice Python fundamentals in a GenAI security context

- Understand how GenAI systems can fail, leak data, or be misused

- Translate technical behavior into risk, impact, and control thinking aligned with GRC principles

## What This Project Does
- Stores GenAI risks using Python lists and dictionaries

- Simulates how risks are accessed, evaluated, and logged

- Connects coding errors (logic, input handling) to real-world GenAI risks

- Introduces basic agentic risk evaluation logic

## Current Focus Areas
- OWASP GenAI risks (e.g., Prompt Injection, Data Leakage, Model Misuse)

- Python control flow (if/else, functions, dictionaries)

- Risk impact reasoning and mitigation mapping

## Status
This is an early learning version (v0.1) and is actively evolving.

The project will grow as I deepen my understanding of:

- OWASP GenAI threats and mitigations

- Secure-by-design AI systems

- GRC and risk engineering workflows

The goal is steady progression from learning artifacts â†’ job-ready security tooling.


## Disclaimer

This project is for educational and learning purposes and does not represent a production security tool.






























## Learning Notes (Week 1)

What this project demonstrates:
A modeled GenAI risk register using Python structures to represent LLM-related risks, impacts, and mitigation controls.

Key insight gained:
Modeling risks as data highlighted why static evidence (e.g., screenshots) cannot represent a real security posture in dynamic AI systems.

Open questions:
How real-world systems detect prompt injection at runtime and how response latency can be exploited before controls engage.
